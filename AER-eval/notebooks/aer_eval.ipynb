{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone2index = {\"sil\": 0, \"spn\": 1, \"sp\": 2, \"OW0\": 3, \"UW1\": 4, \"EY0\": 5, \"AW1\": 6, \"AW0\": 7, \n",
    "               \"AH0\": 8, \"AH2\": 9, \"EY1\": 10, \"AH1\": 11, \"AO1\": 12, \"AY2\": 13, \"EH2\": 14, \n",
    "               \"UW2\": 15, \"K\": 16, \"NG\": 17, \"F\": 18, \"JH\": 19, \"AY0\": 20, \"AO0\": 21, \"M\": 22, \n",
    "               \"CH\": 23, \"IH2\": 24, \"UH0\": 25, \"HH\": 26, \"L\": 27, \"AA2\": 28, \"IH0\": 29, \"R\": 30, \n",
    "               \"TH\": 31, \"AA1\": 32, \"AE2\": 33, \"D\": 34, \"Z\": 35, \"EH1\": 36, \"IH1\": 37, \"OW1\": 38, \n",
    "               \"OY2\": 39, \"DH\": 40, \"AE1\": 41, \"IY1\": 42, \"UH1\": 43, \"AW2\": 44, \"OY1\": 45, \"UH2\": 46, \n",
    "               \"UW0\": 47, \"B\": 48, \"W\": 49, \"AE0\": 50, \"OW2\": 51, \"S\": 52, \"EY2\": 53, \"AO2\": 54, \n",
    "               \"OY0\": 55, \"T\": 56, \"SH\": 57, \"ZH\": 58, \"IY0\": 59, \"AY1\": 60, \"EH0\": 61, \"ER1\": 62,\n",
    "               \"V\": 63, \"IY2\": 64, \"Y\": 65, \"N\": 66, \"ER2\": 67, \"ER0\": 68, \"G\": 69, \"AA0\": 70, \n",
    "               \"P\": 71}\n",
    "\n",
    "phone2index_n = {\"sil\": 0, \"OW\": 1, \"UW\": 2, \"EY\": 3, \"AW\": 4, \"AH\": 5, \"AO\": 6, \"AY\": 7, \"EH\": 8, \n",
    "               \"K\": 9, \"NG\": 10, \"F\": 11, \"JH\": 12, \"M\": 13, \"CH\": 14, \"IH\": 15, \"UH\": 16, \"HH\": 17,\n",
    "               \"L\": 18, \"AA\": 19, \"R\": 20, \"TH\": 21, \"AE\": 22, \"D\": 23, \"Z\": 24, \"OY\": 25, \"DH\": 26, \n",
    "               \"IY\": 27, \"B\": 28, \"W\": 29, \"S\": 30, \"T\": 31, \"SH\": 32, \"ZH\": 33, \"ER\": 34, \"V\": 35, \n",
    "               \"Y\": 36, \"N\": 37, \"G\": 38, \"P\": 39}\n",
    "\n",
    "index2phone = {v: k for k, v in phone2index_n.items()}\n",
    "keys_list = list(phone2index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 12])\n"
     ]
    }
   ],
   "source": [
    "# load reference articulatory for each phoneme\n",
    "ema_ft = torch.load('/home/xuanru/art/art-pos/progress_16000.pt')\n",
    "print(ema_ft.shape) # [40, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_ft[7,:] = ema_ft[5,:] + ema_ft[27,:]  # AY = 0.4AH + 0.6IY\n",
    "ema_ft[3,:] = ema_ft[8,:] + ema_ft[27,:]  # EY = 0.5EH + 0.5IY\n",
    "ema_ft[1,:] = ema_ft[6,:] + ema_ft[2,:]   # OW = 0.6AO + 0.4UW\n",
    "ema_ft[4,:] = ema_ft[19,:] + ema_ft[2,:]  # AW = 0.7AA + 0.3UW\n",
    "ema_ft[25,:] = ema_ft[6,:] + ema_ft[27,:]  # OY = 0.4AO + 0.6IY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 12])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_d: 7.3915696144104\n",
      "row_piar: (2, 28)\n",
      "5.17409873008728\n"
     ]
    }
   ],
   "source": [
    "# define threshold\n",
    "ema = ema_ft[1:] # w/o sil\n",
    "print(ema.shape)\n",
    "\n",
    "max_distance = float('-inf')\n",
    "row_pair = None\n",
    "\n",
    "for i in range(ema.size(0)):\n",
    "    for j in range(i + 1, ema.size(0)):\n",
    "        distance = torch.norm(ema[i] - ema[j], p=2).item()\n",
    "        if distance > max_distance:\n",
    "            max_distance = distance\n",
    "            row_pair = (i, j)\n",
    "\n",
    "print(f\"max_d: {max_distance}\")\n",
    "print(f\"row_piar: {row_pair}\") # 2 28 \n",
    "\n",
    "factor = 0.7 # adjust\n",
    "threshold = max_distance * factor\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ema = ema_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/xuanru/miniconda3/envs/whisperx/lib/python3.10/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/xuanru/miniconda3/envs/whisperx/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from model.encodec import ArticulatoryEncodec\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# dependency - check requirements\n",
    "# download ckpts from https://drive.google.com/drive/u/1/folders/1XICRfrWo7G6EkYKRQafw24DwJh_AbHwz\n",
    "# and place them under '../ckpts'\n",
    "\n",
    "config_file = \"../configs/ep2w_hifigan_libritts_eng.yml\"\n",
    "with open(config_file) as f:\n",
    "     config = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "config['linear_model_path']= Path.cwd().parent/\"ckpts/wavlm_large-9_cut-10_mngu_linear.pkl\"\n",
    "config['generator_ckpt']= Path.cwd().parent/\"ckpts/generator_ep2w_hifigan_multilang.ckpt\"\n",
    "config['spk_ft_ckpt'] = Path.cwd().parent/\"ckpts/spkfc_ep2w_hifigan_multilang.ckpt\"\n",
    "config['device'] = 'cuda:0'\n",
    "device = torch.device(config['device'])\n",
    "\n",
    "encodec = ArticulatoryEncodec(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_frame_per(wav_file, hypo_phn_index, ref_ema, threshold, device):\n",
    "    code = encodec.encode(wav_file)\n",
    "    ema = code['art'][:,:12] # [T, 12]\n",
    "    ema = torch.tensor(ema).to(device)\n",
    "\n",
    "    hypo_phn_art = ref_ema[hypo_phn_index].to(device)\n",
    "\n",
    "    if hypo_phn_art.shape[0] == ema.shape[0]:\n",
    "        T = ema.shape[0]\n",
    "        print(ema.shape)\n",
    "        print(hypo_phn_art.shape)\n",
    "        distances = torch.norm(ema - hypo_phn_art, dim=1, p=2)\n",
    "        neg_mask = distances > threshold\n",
    "\n",
    "        neg_count = neg_mask.sum().item()\n",
    "        neg_ratio = neg_count / T\n",
    "\n",
    "        return neg_ratio\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102, 12])\n",
      "torch.Size([102, 12])\n",
      "AER: 0.9803921568627451%\n"
     ]
    }
   ],
   "source": [
    "wav_file = \"/data/xuanru/VCTK/VCTK_16k/p225/p225_001.wav\"\n",
    "\n",
    "# here I use GT_phn_idx for example, please replace hypo_phn_index with the phn list obtained from your decoding\n",
    "mfa = np.load('/data/xuanru/VCTK/MFA_label_16k/p225/p225_001.npy')\n",
    "mfa = np.vectorize(phone2index_n.get)([''.join(filter(str.isalpha, keys_list[val])) for val in mfa])\n",
    "hypo_phn_index = mfa\n",
    "\n",
    "aer = cal_frame_per(wav_file, hypo_phn_index, ref_ema, threshold, device)\n",
    "print(f\"AER: {aer * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phn-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_phn_aer(hypo_phn_list, gt_phn_list, ref_ema, threshold, device):\n",
    "    errors = 0\n",
    "    total_comparisons = len(gt_phn_list)\n",
    "    \n",
    "    for i in range(total_comparisons):\n",
    "        if i < len(hypo_phn_list) and i < len(gt_phn_list):\n",
    "            phn1 = hypo_phn_list[i]\n",
    "            phn2 = gt_phn_list[i]\n",
    "            art1 = ref_ema[phone2index_n[phn1]].to(device)\n",
    "            art2 = ref_ema[phone2index_n[phn2]].to(device)\n",
    "            \n",
    "            distance = torch.norm(art1 - art2, p=2)\n",
    "            print(distance)\n",
    "            \n",
    "            if distance > threshold:\n",
    "                errors += 1 * threshold/distance\n",
    "        else:\n",
    "            errors += 1\n",
    "    \n",
    "    aer = errors / total_comparisons\n",
    "    return aer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.58704936504364\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(2.5988, device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(6.1105, device='cuda:1')\n",
      "tensor(2.1792, device='cuda:1')\n",
      "aer: 20.26929473876953%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "hypo_phn_list = ['P', 'L', 'EY', 'Z', 'K', 'K', 'AW', 'L']\n",
    "gt_phn_list = ['P', 'L', 'IY', 'Z', 'K', 'AO', 'L']\n",
    "\n",
    "threshold_phn = max_distance * 0.35\n",
    "print(threshold_phn)\n",
    "\n",
    "aer = calculate_phn_aer(hypo_phn_list, gt_phn_list, ref_ema, threshold_phn, device)\n",
    "print(f\"aer: {aer * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
